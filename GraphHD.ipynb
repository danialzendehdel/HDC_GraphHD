{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c220a4d887f0005",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-07T23:02:33.477966Z",
     "start_time": "2024-05-07T23:02:12.814772Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!pip install bhv\n",
    "!pip install torch_geometric\n",
    "!pip install torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cf9276f42607d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T20:16:19.149020Z",
     "start_time": "2024-05-06T20:16:08.929355Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!pip install torch-hd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5674c359810556d8",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## GraphHD\n",
    "\n",
    "- Contrary to the usual ML problems in which data is captured in the Euclidean space such as vector of feature, images, time series data, etc., in real world scenarios `representation of relationships between entities` has the importance. These information about such entities and the relationships between them is non-euclidean and can be represented as a graph.\n",
    "- However, the graph data is not directly usable by the ML models. The graph data needs to be converted into a format that can be used by the ML models. This conversion is called `graph embedding` or `graph encoding`.\n",
    "- GraphHD is a graph encoding technique that uses Hypervector Distributed Computing (HDC) to encode the graph data into a format that can be used by the ML models.\n",
    "- HDC represents information in a high- dimensional space using hypervectors. Each hypervector stores data holographically, that is, each dimension contains the same amount of information. This allows HDC to store and process large amounts of data in a compact and efficient manner, as the information is not concentrated in a single dimension but is spread out across all dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453af00c3f7927f1",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Hyperdimensional Computing\n",
    "\n",
    "- Hyperdimensional computing seeks to emulate brain circuits in a more robust and efficient way than neural networks by representing information as points in a high-dimensional space, called $\\textbf{hypervectors}$. Hypervectors are typically binary or bipolar vectors with ten thousand dimensions.\n",
    "\n",
    "\n",
    "#### HDC models\n",
    "- HDC models can often be separated into three stages: encoding, training, and inference. An overview of HDC classification is shown in Figure 1. The encoding stage is application specific and serves to transform the input data into hypervectors. During the training the hypervectors are aggregated to learn a model. Finally, inferences into the model can be made using the generated class representations.\n",
    "\n",
    "![Figure 1](images/fig1.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b2d7e7cdac51b8",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Process \n",
    "\n",
    "- Encoding \n",
    "    - The mapping of data to the high-dimensional space is the first step in HDC and this process corresponds to encoding.\n",
    "    $$Enc: \\mathbb{I} \\rightarrow \\mathbb{H}^d$$\n",
    "\n",
    "Encoding is the HDC counterpart to the feature extraction process in classical learning methods. Thus, the main intuitive principle that governs the encoding is that inputs that are similar in the original space should be mapped to similar hypervectors.\n",
    "The process starts by generating a set of basis hypervectors that represent units of information (e.g. feature values and positions). The basis hypervectors remain fixed throughout computation and each data sample is encoded by combining and manipulating them using the addition (bundling), multiplication (binding) and permutation operations.\n",
    "- Record based encoding\n",
    "The encoding generates the hypervector $\\textbf{H}$ from the randomly generated key hypervectors $\\textbf{K}$  which are bound to their value $\\textbf{V^\\bar_i}$ which is one of predifined value hypervectors in $\\textbf{V}$.\n",
    "\n",
    "$$ \\textbf{H} = [K_1 \\times \\textbf{V^\\bar_1} + K_2 \\times \\textbf{V^\\bar_2} + .... + k_N \\times \\textbf{V^\\bar_N}  ]$$\n",
    "        \n",
    "\n",
    "\n",
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10167ec29cfeb57",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "- Training \n",
    "A trained HDC model creates $\\textit{k}$ hypervector one for each class therefore  the model $$ M = \\{ C_1, C_2, ..., C_k\\}$$ where $C_i$ is the hypervector for class $i$ and contains all the information used to identify the i-th class.\n",
    "- Each $C_i$ is calculated as the vector with the smallest average distance to the the hypervectors obtained by encoding the training samples of class i:\n",
    "$$ C_i = \\sum_{j:\\l(x_j)=i} Enc(x_j)$$\n",
    "\n",
    "In which $\\l(x_j)$ is the label of the j-th training sample and $Enc(x_j)$ is the hypervector obtained by encoding the j-th training sample and the sum  is bundling. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf09be1722f29b26",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### GraphHD Encoding\n",
    "$$Enc_{v} : V(G) \\rightarrow \\mathbb{R}^{d}$$\n",
    "\n",
    "By $\\textit{encoding}$  a random hypervector would be assigned to each vertice of the graph $G$ denoted by $V(G)$. For example, one for each letter to encode text.\n",
    "\n",
    "\n",
    "$\\textit{How to encode a graph?} \\\\$ \n",
    "\n",
    "In other structures like text, there is a correspondence between the letters of different texts. For example, the letter 'a' in the word 'apple' is the same as the letter 'a' in the word 'banana' which makes it reasonable to encode the letter 'a' with the same hypervector in both words. Because of structure of a graph, there is no correspondence between vertices of different graphs. \n",
    "- Vertex Identity Is Contextual: In graphs, vertices do not have a universal \"meaning\" or identity outside their specific graph context. The identity of a vertex is often defined by its position and its relationships within a graph, not by any inherent, standalone attribute.\n",
    "- Lack of Universal Features: Unlike text where 'A' always carries the same semantic meaning, a vertex in one graph does not inherently carry over the same attributes or relationships as a similarly positioned vertex in another graph. For instance, a vertex representing a city in one graph might be central, while in another graph, a similarly positioned vertex could represent a minor town.\n",
    "\n",
    "\n",
    "$\\textit{Extracting an identifier for the vertices based only on the topology of the graph}$    \\\n",
    "$\\textbf{PageRank}$\n",
    "\n",
    "PageRank centerality metric receives a graph as input and returns, for each vertex $v_i \\in V$ a value $c(v_i) \\in [0, 1]$  that measures its “importance” in the graph.\n",
    "\n",
    "From this ranking induced by the PageRank centrality of the vertices, it is possible to establish a meaningful connection between vertices in different graphs. Therefore, GraphHD uses the centrality rank of the vertex as its identifier (or symbol). Accordingly, vertices of different graphs, but with the same centrality rank, are encoded to the same random hypervector from the basis set.\n",
    "\n",
    "\n",
    "After creating the hypervectors for each vertex, GraphHD makes use of these representations to also encode each edge $(v_i, v_j) \\in E(G)$, The edge encoding function $Enc_e$ is defined as follows:\n",
    "\n",
    "$Enc_e((v_j, v_j)) = Enc_v(v_i) \\times Enc_v(v_j)$\n",
    "\n",
    "\n",
    "\n",
    "The $\\times$ symbol represents the binding operation in HDC, which is the standard operation to represent an association between a pair of hypervectors, similar to the role of an edge in a graph. The result of the binding operation is a third vector, statistically quasi-orthogonal to the operand vectors, which we name edge-hypervectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6506525c1d79bc70",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T19:56:23.223034Z",
     "start_time": "2024-05-08T19:56:13.760612Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from statistics import fmean, stdev\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from functools import reduce\n",
    "from operator import add\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch_geometric.datasets import TUDataset\n",
    "import torchmetrics\n",
    "import torchhd\n",
    "print(torchhd.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631aff78324a8975",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### TUDataset\n",
    "\n",
    "Link to [TUDataset](https://chrsmrrs.github.io/datasets/docs/home/). \n",
    "Total number of graphs in `MUTAG` is 188. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598d04a2a78a537c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T19:56:23.240642Z",
     "start_time": "2024-05-08T19:56:23.225767Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for other available datasets see: https://pytorch-geometric.readthedocs.io/en/latest/notes/data_cheatsheet.html?highlight=tudatasets\n",
    "dataset = \"MUTAG\"\n",
    "\n",
    "graphs = TUDataset(\"../data\", dataset)\n",
    "print(f\"Total number of graphs: {len(graphs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ccc1708ee3d735",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T19:56:23.248991Z",
     "start_time": "2024-05-08T19:56:23.241930Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "graphs[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ef9b785afe4c51",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T19:56:23.466045Z",
     "start_time": "2024-05-08T19:56:23.250103Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_graph(graph):\n",
    "    # Convert to networkx graph\n",
    "    G = nx.Graph()\n",
    "    edge_index = graph.edge_index.numpy()\n",
    "    # Add edges and nodes to the graph\n",
    "    G.add_edges_from(edge_index.T)\n",
    "    # Draw the graph\n",
    "    nx.draw(G, with_labels=True, node_color='lightblue')\n",
    "    plt.show()\n",
    "\n",
    "# Visualize the first graph\n",
    "plot_graph(graphs[2])\n",
    "\n",
    "data = graphs[2]\n",
    "print(f\"Number of nodes: {data.num_nodes}\")\n",
    "print(f\"Number of edges: {data.num_edges}\")\n",
    "print(f\"Number of node features: {data.num_node_features}\")\n",
    "# ===========================================================\n",
    "# Each node has 7 features. These features typically represent properties or characteristics of the nodes which can be used for graph neural network tasks. For instance, in a chemical graph like MUTAG, these could include properties like atom type, charge, etc.\n",
    "# ===========================================================\n",
    "print(f\"Number of graph features: {data.num_features}\")\n",
    "print(f\"Is the graph directed: {data.is_directed()}\")\n",
    "print(f\"Graph label (class): {data.y}\")\n",
    "# ===========================================================\n",
    "# In classification tasks, each graph is assigned a label, which in the case of MUTAG could indicate the presence or absence of mutagenic effect.\n",
    "# ===========================================================\n",
    "\n",
    "# print(f\"Edge index (connectivity): \\n{data.edge_index}\")\n",
    "print(f\"Edge index shape: \")\n",
    "df = pd.DataFrame(data.edge_index)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d876f4d9086b87e2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T19:56:23.477267Z",
     "start_time": "2024-05-08T19:56:23.470184Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sparse_stochastic_graph(G):\n",
    "    \"\"\"\n",
    "    Returns a sparse adjacency matrix of the graph G.\n",
    "    The values indicate the probability of leaving a vertex.\n",
    "    This means that each column sums up to one.\n",
    "    \"\"\"\n",
    "    rows, columns = G.edge_index # rows containing source nodes, and columns containing destination nodes of each edge.\n",
    "    # Calculate the probability for each column\n",
    "    values_per_column = 1.0 / torch.bincount(columns, minlength=G.num_nodes) # probability of leaving a vertex\n",
    "    values_per_edge = values_per_column[columns]\n",
    "    size = (G.num_nodes, G.num_nodes)\n",
    "    return torch.sparse_coo_tensor(G.edge_index, values_per_edge, size) # The sparse stochastic adjacency matrix is a representation of the graph where each                                                                        entry M[i][j] represents the probability of transitioning from node i to node j.\n",
    "\n",
    "\n",
    "def inverse_permutation(perm):\n",
    "    inv = torch.empty_like(perm)\n",
    "    inv[perm] = torch.arange(perm.size(0), device=perm.device)\n",
    "    return inv\n",
    "\n",
    "\n",
    "def to_undirected(edge_index):\n",
    "    \"\"\"\n",
    "    Returns the undirected edge_index\n",
    "    [[0, 1], [1, 0]] will result in [[0], [1]]\n",
    "    \"\"\"\n",
    "    edge_index = edge_index.sort(dim=0)[0]\n",
    "    edge_index = torch.unique(edge_index, dim=1)\n",
    "    return edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6ba5cf014081ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T19:56:23.492611Z",
     "start_time": "2024-05-08T19:56:23.479147Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def validate_stochastic_matrix(G):\n",
    "    sparse_matrix = sparse_stochastic_graph(G)\n",
    "    dense_matrix = sparse_matrix.to_dense()\n",
    "    # print(dense_matrix)\n",
    "    column_sums = dense_matrix.sum(dim=0)\n",
    "    print(\"Column sums:\", column_sums)  # Each element should be very close to 1\n",
    "    return column_sums\n",
    "\n",
    "validate_stochastic_matrix(graphs[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff558946fe048d3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T19:56:23.505453Z",
     "start_time": "2024-05-08T19:56:23.494509Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pagerank(G, alpha=0.85, max_iter=100):\n",
    "    \"\"\"\n",
    "    Calculate PageRank based on the sparse stochastic graph representation.\n",
    "    \"\"\"\n",
    "    N = G.num_nodes\n",
    "    M = sparse_stochastic_graph(G) * alpha\n",
    "    v = torch.full((N,), 1 / N)\n",
    "    p = torch.full((N,), (1 - alpha) / N)\n",
    "    for _ in range(max_iter):\n",
    "        v = M @ v + p\n",
    "    return v\n",
    "pagerank(graphs[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382ff9bdde112aae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T19:56:27.021903Z",
     "start_time": "2024-05-08T19:56:27.015231Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_vectors, vector_size=47):\n",
    "        super(Encoder, self).__init__()\n",
    "        # Generate random binary hypervectors\n",
    "        self.node_ids = nn.Parameter(self.generate_random_hypervectors(num_vectors, vector_size), requires_grad=False)\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_random_hypervectors(num_vectors, vector_size):\n",
    "        # Generate random 0s and 1s\n",
    "        return torch.randint(0, 2, (num_vectors, vector_size), dtype=torch.int32)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Assuming 'pagerank' and 'inverse_permutation' functions are defined elsewhere\n",
    "        pr = pagerank(x)\n",
    "        pr_argsort = inverse_permutation(torch.argsort(pr))\n",
    "\n",
    "        node_id_hvs = self.node_ids[pr_argsort]\n",
    "\n",
    "        # Assuming 'to_undirected' function is defined elsewhere\n",
    "        row, col = to_undirected(x.edge_index)\n",
    "\n",
    "        # Perform XOR binding for each edge\n",
    "        hvs = [node_id_hvs[s] ^ node_id_hvs[t] for s, t in zip(row, col)]\n",
    "\n",
    "        # Bundle hypervectors using majority rule (here simplified to mean)\n",
    "        if len(hvs) > 0:\n",
    "            graph_hv = hvs[0]\n",
    "            for hv in hvs[1:]:\n",
    "                graph_hv = torchhd.bundle(graph_hv, hv)\n",
    "        else:\n",
    "            # Handle case where there are no edges\n",
    "            graph_hv = torch.zeros(self.vector_size, dtype=torch.float32)\n",
    "        return graph_hv.type(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6932f6b12ae0e521",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T19:56:51.438258Z",
     "start_time": "2024-05-08T19:56:51.432137Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MajorityClassification(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MajorityClassification, self).__init__()\n",
    "        self.class_vectors = {}\n",
    "        self.classes = []  # To maintain the order of classes\n",
    "\n",
    "    def add(self, x, c):\n",
    "        \"\"\"Adds or updates the class vector for class c with a new training hypervector x.\"\"\"\n",
    "        if c not in self.class_vectors:\n",
    "            self.class_vectors[c] = x\n",
    "            self.classes.append(c)\n",
    "        else:\n",
    "            # Bundle the new hypervector with the existing class vector\n",
    "            self.class_vectors[c] = torchhd.bundle(self.class_vectors[c], x)\n",
    "    def num_classes(self):\n",
    "        return len(self.classes)\n",
    "    \n",
    "    def get_class_info(self):\n",
    "        return {c: self.class_vectors[c].detach().clone() for c in self.classes}  # Using detach().clone() to safely export tensors without affecting computation graph\n",
    "\n",
    "            \n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Classifies an input hypervector x by finding the class with the highest similarity class vector.\"\"\"\n",
    "        # Gather all class vectors into a single tensor for batch processing\n",
    "        class_vectors = torch.stack([self.class_vectors[c] for c in self.classes])\n",
    "        # Calculate Hamming similarities between the input vector and all class vectors\n",
    "        similarities = torchhd.hamming_similarity(x, class_vectors)\n",
    "        # Find the index of the class vector with the highest similarity\n",
    "        max_index = torch.argmax(similarities)\n",
    "        # Return the class corresponding to the highest similarity\n",
    "        return self.classes[max_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13bbea38d8efdf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T19:56:52.636315Z",
     "start_time": "2024-05-08T19:56:52.628602Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classifier = MajorityClassification()\n",
    "# Simulate adding training data\n",
    "classifier.add(torchhd.random(1, 10), 'class1')\n",
    "classifier.add(torchhd.random(1, 10), 'class2')\n",
    "classifier.add(torchhd.random(1, 10), 'class3')\n",
    "\n",
    "# Classify a new hypervector\n",
    "new_hv = torchhd.random(1, 10)\n",
    "predicted_class = classifier(new_hv)\n",
    "print(f\"The predicted class for the new hypervector is: {predicted_class}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0af7d0778fa5a1a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T19:56:54.306571Z",
     "start_time": "2024-05-08T19:56:54.304142Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8759d9e84757ee1a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T19:56:59.620253Z",
     "start_time": "2024-05-08T19:56:54.832570Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_graph_size = max(g.num_nodes for g in graphs)\n",
    "folds = torch.utils.data.random_split(graphs, [.1]*10)\n",
    "accuracies = []\n",
    "\n",
    "for i in range(10):\n",
    "    train_ld, test_ld = reduce(add, [folds[j] for j in range(10) if j != i]), folds[i]\n",
    "    \n",
    "    encode = Encoder(max_graph_size, vector_size=47)\n",
    "    \n",
    "    model = MajorityClassification()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for samples in tqdm(train_ld, desc=f\"Training fold {i + 1}\"):\n",
    "            samples_hv = encode(samples)\n",
    "            model.add(samples_hv, samples.y.item())\n",
    "            \n",
    "        accuracy = torchmetrics.Accuracy(\"multiclass\", num_classes=graphs.num_classes)\n",
    "        \n",
    "        for samples in tqdm(test_ld, desc=f\"Testing fold {i + 1}\"):\n",
    "            samples_hv = encode(samples)\n",
    "            output = model(samples_hv)\n",
    "            accuracy.update(torch.IntTensor([output]), samples.y)\n",
    "            \n",
    "    acc = accuracy.compute().item()\n",
    "    accuracies.append(acc)\n",
    "    print(f\"Testing accuracy of {acc*100:.3f}% for fold {i + 1}\")\n",
    "    \n",
    "print(f\"tenfold crossvalidation accuracy mean: {fmean(accuracies)}, stdev: {stdev(accuracies)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b68db399f5f8d9e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T19:57:01.534895Z",
     "start_time": "2024-05-08T19:57:01.524493Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_hv = torchhd.random(1, 47)  # Example new sample\n",
    "print(new_hv)\n",
    "predicted_class = model(new_hv)\n",
    "\n",
    "print(f\"The predicted class for the new hypervector is: {predicted_class}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40db3cc212613ee9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T19:57:01.896918Z",
     "start_time": "2024-05-08T19:57:01.886584Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for label, hypervector in model.get_class_info().items():\n",
    "    print(f\"Class {label}: Hypervector - {hypervector}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f14c0a09703d53",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
